{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Heart Disease Prediction\n",
    "\n",
    "This notebook focuses on preparing the heart disease dataset for model training by applying various feature engineering techniques to enhance model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('../data/heart.csv')\n",
    "\n",
    "# Make a copy of the data to avoid modifying the original\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# For demonstration, let's assume there might be missing values and handle them\n",
    "# Use median for numerical features\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "df[numerical_cols] = numerical_imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Use the most frequent value for categorical features\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Let's create some new features that might help the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age groups\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 40, 50, 60, 100], labels=[0, 1, 2, 3])\n",
    "\n",
    "# Create BMI proxy (if height and weight were available)\n",
    "# Since we don't have height and weight, we'll skip this\n",
    "\n",
    "# Create cholesterol ratio (Total Cholesterol / HDL)\n",
    "# Since we don't have HDL, we'll use a rough approximation\n",
    "df['chol_normalized'] = df['chol'] / df['chol'].mean()\n",
    "\n",
    "# Create a feature for blood pressure category\n",
    "def bp_category(x):\n",
    "    if x < 120: return 0  # Normal\n",
    "    elif x < 130: return 1  # Elevated\n",
    "    elif x < 140: return 2  # Stage 1 hypertension\n",
    "    else: return 3  # Stage 2 hypertension\n",
    "\n",
    "df['bp_category'] = df['trestbps'].apply(bp_category)\n",
    "\n",
    "# Heart rate reserve (approximation)\n",
    "df['max_heart_rate'] = 220 - df['age']\n",
    "df['heart_rate_reserve'] = df['max_heart_rate'] - df['thalach']\n",
    "df['heart_rate_percentage'] = df['thalach'] / df['max_heart_rate']\n",
    "\n",
    "# Interaction terms\n",
    "df['age_chol_interaction'] = df['age'] * df['chol_normalized']\n",
    "df['exercise_bp_interaction'] = df['exang'] * df['bp_category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply standard scaling to numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_cols_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', \n",
    "                           'heart_rate_reserve', 'heart_rate_percentage', 'age_chol_interaction']\n",
    "\n",
    "X_train[numerical_cols_to_scale] = scaler.fit_transform(X_train[numerical_cols_to_scale])\n",
    "X_test[numerical_cols_to_scale] = scaler.transform(X_test[numerical_cols_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed training and testing datasets\n",
    "X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "# Save the scaler for future use\n",
    "import joblib\n",
    "joblib.dump(scaler, '../data/processed/scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Let's use a simple model to get a preliminary idea of feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Fit a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# View feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we performed feature engineering on the heart disease dataset to prepare it for model training. We:\n",
    "\n",
    "1. Handled missing values\n",
    "2. Created new features based on domain knowledge\n",
    "3. Normalized numerical features\n",
    "4. Split the data into training and testing sets\n",
    "5. Analyzed feature importance\n",
    "\n",
    "The processed data is now ready for model training in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
