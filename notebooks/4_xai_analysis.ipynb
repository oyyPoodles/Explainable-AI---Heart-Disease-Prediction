{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction - Explainable AI (XAI) Analysis\n",
    "\n",
    "This notebook implements and demonstrates XAI techniques for heart disease prediction model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset, model, and scaler\n",
    "df = pd.read_csv('../data/heart.csv')\n",
    "model = joblib.load('../backend/model/model.pkl')\n",
    "scaler = joblib.load('../backend/model/scaler.pkl')\n",
    "feature_names = joblib.load('../backend/model/feature_names.pkl')\n",
    "\n",
    "# Split the data\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Scale the features\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "print(f\"Model loaded: {type(model).__name__}\")\n",
    "print(f\"Dataset shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global Explanations with SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values show the impact of each feature on model predictions across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize SHAP explainer\n",
    "# For tree-based models (Random Forest, XGBoost)\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_scaled)\n",
    "    \n",
    "    # For binary classification, some models return a list with one element\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # Get SHAP values for the positive class (heart disease)\n",
    "else:\n",
    "    # For other models (e.g., Logistic Regression)\n",
    "    explainer = shap.KernelExplainer(model.predict_proba, shap.sample(X_scaled, 100))\n",
    "    shap_values = explainer.shap_values(X_scaled)[1]  # For the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Global feature importance based on SHAP values\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_scaled, feature_names=feature_names, plot_type=\"bar\")\n",
    "plt.title('Global Feature Importance (SHAP)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# SHAP summary plot showing feature value impacts\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_scaled, feature_names=feature_names)\n",
    "plt.title('Feature Value Impact on Predictions (SHAP)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Local Explanations with SHAP\n",
    "\n",
    "Let's examine individual predictions to understand how the model arrives at specific decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select a sample patient for explanation\n",
    "sample_idx = 50  # Can be changed to examine different patients\n",
    "sample_X = X.iloc[sample_idx:sample_idx+1]\n",
    "sample_X_scaled = scaler.transform(sample_X)\n",
    "true_label = y.iloc[sample_idx]\n",
    "prediction = model.predict(sample_X_scaled)[0]\n",
    "prediction_proba = model.predict_proba(sample_X_scaled)[0, 1]\n",
    "\n",
    "print(f\"Sample patient data:\\n{sample_X}\")\n",
    "print(f\"True label: {'Heart Disease' if true_label == 1 else 'No Heart Disease'}\")\n",
    "print(f\"Predicted: {'Heart Disease' if prediction == 1 else 'No Heart Disease'} with {prediction_proba:.4f} probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# SHAP force plot for the sample patient\n",
    "plt.figure(figsize=(20, 3))\n",
    "shap_values_sample = explainer.shap_values(sample_X_scaled)\n",
    "\n",
    "# For binary classification, some models return a list\n",
    "if isinstance(shap_values_sample, list):\n",
    "    shap_values_sample = shap_values_sample[1]  # For the positive class\n",
    "\n",
    "shap.force_plot(explainer.expected_value if not hasattr(explainer, 'expected_value') else explainer.expected_value[1], \n",
    "                shap_values_sample, \n",
    "                sample_X, \n",
    "                feature_names=feature_names, \n",
    "                matplotlib=True,\n",
    "                show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert force plot to decision plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.decision_plot(explainer.expected_value if not hasattr(explainer, 'expected_value') else explainer.expected_value[1], \n",
    "                   shap_values_sample, \n",
    "                   feature_names=feature_names)\n",
    "plt.title('SHAP Decision Plot for Sample Patient')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LIME Explanations\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) creates a simple, interpretable model that approximates the original model's behavior locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize LIME explainer\n",
    "lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "    X_scaled,\n",
    "    feature_names=feature_names,\n",
    "    class_names=['No Heart Disease', 'Heart Disease'],\n",
    "    mode='classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate LIME explanation for sample patient\n",
    "lime_exp = lime_explainer.explain_instance(\n",
    "    sample_X_scaled[0],\n",
    "    model.predict_proba,\n",
    "    num_features=len(feature_names),\n",
    "    top_labels=1\n",
    ")\n",
    "\n",
    "# Plot LIME explanation\n",
    "plt.figure(figsize=(10, 8))\n",
    "lime_exp.as_pyplot_figure(label=1)  # For the positive class\n",
    "plt.title('LIME Explanation for Sample Patient')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing XAI Techniques\n",
    "\n",
    "Now let's compare the explanations from both SHAP and LIME for the same patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract feature importance from LIME explanation\n",
    "lime_importance = dict(lime_exp.as_list(label=1))\n",
    "lime_df = pd.DataFrame({\n",
    "    'Feature': [item.split(' ')[0] for item in lime_importance.keys()],\n",
    "    'Importance': list(lime_importance.values())\n",
    "}).sort_values(by='Importance', key=abs, ascending=False)\n",
    "\n",
    "# Extract SHAP values for the sample\n",
    "shap_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': shap_values_sample[0]\n",
    "}).sort_values(by='Importance', key=abs, ascending=False)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# SHAP plot\n",
    "sns.barplot(x='Importance', y='Feature', data=shap_df.head(10), ax=axes[0])\n",
    "axes[0].set_title('Top 10 Features (SHAP)')\n",
    "\n",
    "# LIME plot\n",
    "sns.barplot(x='Importance', y='Feature', data=lime_df.head(10), ax=axes[1])\n",
    "axes[1].set_title('Top 10 Features (LIME)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing a Function for Explanation\n",
    "\n",
    "Finally, let's develop a function that can be used in the backend API to generate explanations for any patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_explanation(patient_data, model, scaler, feature_names):\n",
    "    \"\"\"\n",
    "    Generate SHAP and LIME explanations for a patient's prediction.\n",
    "    \n",
    "    Args:\n",
    "        patient_data (pd.DataFrame): Patient features as a DataFrame\n",
    "        model: Trained ML model\n",
    "        scaler: Feature scaler used during training\n",
    "        feature_names (list): List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing prediction and explanations\n",
    "    \"\"\"\n",
    "    # Scale the input data\n",
    "    patient_data_scaled = scaler.transform(patient_data)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(patient_data_scaled)[0]\n",
    "    probability = model.predict_proba(patient_data_scaled)[0, 1]\n",
    "    \n",
    "    # Initialize SHAP explainer\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "    else:\n",
    "        # Use a subset of the training data as background\n",
    "        background_data = shap.sample(X_scaled, 100)\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, background_data)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer.shap_values(patient_data_scaled)\n",
    "    \n",
    "    # For binary classification, some models return a list\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # For the positive class\n",
    "    \n",
    "    # Get SHAP values as dictionary\n",
    "    shap_dict = {feature: float(value) for feature, value in zip(feature_names, shap_values[0])}\n",
    "    \n",
    "    # Initialize LIME explainer\n",
    "    lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "        X_scaled,\n",
    "        feature_names=feature_names,\n",
    "        class_names=['No Heart Disease', 'Heart Disease'],\n",
    "        mode='classification'\n",
    "    )\n",
    "    \n",
    "    # Generate LIME explanation\n",
    "    lime_exp = lime_explainer.explain_instance(\n",
    "        patient_data_scaled[0],\n",
    "        model.predict_proba,\n",
    "        num_features=len(feature_names),\n",
    "        top_labels=1\n",
    "    )\n",
    "    \n",
    "    # Extract LIME explanation as list of (feature, weight) tuples\n",
    "    lime_list = lime_exp.as_list(label=1)\n",
    "    lime_explanation = [{'feature': item[0], 'weight': float(item[1])} for item in lime_list]\n",
    "    \n",
    "    # Get global feature importance if available\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = {feature: float(importance) \n",
    "                              for feature, importance in zip(feature_names, model.feature_importances_)}\n",
    "    else:\n",
    "        # For models without built-in feature importance (e.g., Logistic Regression)\n",
    "        feature_importance = {feature: float(abs(coef)) \n",
    "                              for feature, coef in zip(feature_names, model.coef_[0])}\n",
    "    \n",
    "    # Create explanation dictionary\n",
    "    explanation = {\n",
    "        'prediction': int(prediction),\n",
    "        'probability': float(probability),\n",
    "        'shap_values': shap_dict,\n",
    "        'feature_importance': feature_importance,\n",
    "        'lime_explanation': lime_explanation,\n",
    "        'expected_value': float(explainer.expected_value if not hasattr(explainer, 'expected_value') \n",
    "                              else explainer.expected_value[1])\n",
    "    }\n",
    "    \n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the explanation function with our sample patient\n",
    "explanation = generate_explanation(sample_X, model, scaler, feature_names)\n",
    "\n",
    "# Print the explanation in a readable format\n",
    "print(f\"Prediction: {'Heart Disease' if explanation['prediction'] == 1 else 'No Heart Disease'}\")\n",
    "print(f\"Probability: {explanation['probability']:.4f}\")\n",
    "print(\"\\nTop 5 SHAP values:\")\n",
    "for feature, value in sorted(explanation['shap_values'].items(), key=lambda x: abs(x[1]), reverse=True)[:5]:\n",
    "    print(f\"  {feature}: {value:.4f}\")\n",
    "    \n",
    "print(\"\\nTop 5 LIME features:\")\n",
    "for item in sorted(explanation['lime_explanation'], key=lambda x: abs(x['weight']), reverse=True)[:5]:\n",
    "    print(f\"  {item['feature']}: {item['weight']:.4f}\")\n",
    "    \n",
    "print(\"\\nTop 5 global feature importance:\")\n",
    "for feature, value in sorted(explanation['feature_importance'].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {feature}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the explanation function for use in the backend\n",
    "import inspect\n",
    "function_code = inspect.getsource(generate_explanation)\n",
    "\n",
    "with open('../backend/model/explainability.py', 'w') as f:\n",
    "    f.write(\"import pandas as pd\\n\")\n",
    "    f.write(\"import numpy as np\\n\")\n",
    "    f.write(\"import shap\\n\")\n",
    "    f.write(\"from lime import lime_tabular\\n\\n\")\n",
    "    f.write(function_code)\n",
    "\n",
    "print(\"Explanation function saved to '../backend/model/explainability.py'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XAI Analysis Summary\n",
    "\n",
    "In this notebook, we've implemented and analyzed several Explainable AI techniques for our heart disease prediction model:\n",
    "\n",
    "1. **SHAP (Global Explanations)**: We identified which features are most important overall for predicting heart disease.\n",
    "\n",
    "2. **SHAP (Local Explanations)**: We examined how specific feature values contribute to individual predictions through force plots and decision plots.\n",
    "\n",
    "3. **LIME Explanations**: We generated local explanations using LIME, which creates simple approximations of the model for individual predictions.\n",
    "\n",
    "4. **Comparison**: We compared SHAP and LIME explanations, noting similarities and differences in their interpretations.\n",
    "\n",
    "5. **API Integration**: We created a reusable function that can be integrated into our backend API to generate explanations for any new patient.\n",
    "\n",
    "These XAI techniques provide transparency into our model's decision-making process, helping both healthcare providers and patients understand the factors driving heart disease risk predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 }
}